\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{mathtools}
\usepackage{graphicx}
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\DeclareMathOperator{\Tr}{Tr}

%SetFonts

%SetFonts
\begin{document}

\title{Bi-Directional Training in Interference Network}
\author{Shao-Han Chen}
%\date{}							% Activate to display a given date or no date


\maketitle
%\section{}
%\subsection{}

\section{System Model 1: Private and Common Messages}

Under this model, we want to see how cooperation on transmitter(base station) side will improve the channel capacity. One way we implement the cooperation scheme is to make all transmitters for common messages, instead of working separately, a big cooperative array. That is to say, for the backward training part, we \textbf{minimize the mean square error(MSE) of total received signals together}. However, we found this method is actually \textbf{worse} than the one without cooperation. \textbf{How Doesn't this Work?} This is related the way we \textbf{measure performance}. Our objective is to maximize the \textbf{sum capacity} of individual users, which is equal to minimize $\displaystyle\sum_{k} MSE_{k}$. However, our cooperation scheme actually leads to the solution of maximizing total capacity, that is minimize $MSE$. Please note that we are not claiming cooperations imply worse performance. It's just this particular cooperation scheme doesn't work. 
\subsection{System Model}

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{forward_channel}}
    \caption{Forward Channel}
\end{figure} 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{backward_channel}}
    \caption{Backward Channel}
\end{figure} 

\newpage

\subsection{Optimization Problem}
\begin{align*}
\min_{\textbf{v}_{k} ,\textbf{g}_{k}} \displaystyle\sum_{k} MSE^{(c)}_{k}+MSE^{(p)}_{k}
\end{align*}

\begin{align*}
\text{subject to}  \ 	\|	\textbf{v}^{(c)}_{k}	\|^{2}	+\|	\textbf{v}^{(p)}_{k}\|^{2}= P; \| \textbf{g}_{k}	\|^{2} = P
\end{align*}

where
\begin{align*}
MSE^{(c)}_{k} = \mathrm{E}	[	(	x-\textbf{g}^{H(c)}_{k}	\textbf{y}_{k}	)(x-\textbf{g}^{H(c)}_{k}	\textbf{y}_{k})^{H}	]
\end{align*}

\begin{align*}
MSE^{(p)}_{k} = \mathrm{E}	[	(	x^{(p)}_{k}-\textbf{g}^{H(p)}_{k}	\textbf{y}_{k}	)(x^{(p)}_{k}-\textbf{g}^{H(p)}_{k}	\textbf{y}_{k})^{H}	]
\end{align*}





\subsection{The received signal vector at k-th receiver}
\begin{align*}
\textbf{y}_{k} &= \textbf{H}_{kk}
			(\textbf{v}^{(c)}_{k}x
			+\textbf{v}^{(p)}_{k}x^{(p)}_{k})
			+\displaystyle\sum_{j \neq k}\textbf{H}_{kj}(\textbf{v}^{(c)}_{j}x+\textbf{v}^{(p)}_{j}x^{(p)}_{j})
			+\textbf{n}_{k}
\end{align*}

\subsection{SINR Derivation}
\begin{align*}
s^{(c)}_{k} = \textbf{g}^{H(c)}_{k}
		(\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(c)}_{i}x)
\end{align*}

\begin{align*}
s^{(p)}_{k} = \textbf{g}^{H(p)}_{k}
		(\textbf{H}_{kk} 
		\textbf{v}^{(p)}_{k}x^{(p)}_{k})
\end{align*}

\begin{align*}
n^{(c)}_{k} = \textbf{g}^{H(c)}_{k}
		(\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(p)}_{i}x^{(p)}_{i}
		+\textbf{n}_{k})
\end{align*}

\begin{align*}
n^{(p)}_{k} = \textbf{g}^{H(p)}_{k}
		(\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(c)}_{i}x
		+\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(p)}_{j}x^{(p)}_{j}
		+\textbf{n}_{k})
\end{align*}

\begin{align*}
\frac	{	|s^{(c)}_{k}|^2	}{	|n^{(c)}_{k}|^2	} = 
\frac {	|\textbf{g}^{H(c)}_{k}
		\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(c)}_{i}|^2	
	} 
	{	\displaystyle\sum_{i}
		|\textbf{g}^{H(c)}_{k}
		\textbf{H}_{ki} 
		\textbf{v}^{(p)}_{i}|^2
		+|\textbf{g}^{H(c)}_{k}
		\textbf{R}_{k}
		\textbf{g}^{(c)}_{k}|
	}
\end{align*}

\begin{align*}
\frac	{	|s^{(p)}_{k}|^2	}{	|n^{(p)}_{k}|^2	} = 
\frac {|\textbf{g}^{H(p)}_{k}
		\textbf{H}_{kk} 
		\textbf{v}^{(p)}_{k}|^2	
	} 
	{	|\textbf{g}^{H(p)}_{k}
		\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(c)}_{i}|^2
		+\displaystyle\sum_{j \neq k}
		|\textbf{g}^{H(p)}_{k}
		\textbf{H}_{kj} 
		\textbf{v}^{(p)}_{j}|^2
		+|\textbf{g}^{H(p)}_{k}
		\textbf{R}_{k}
		\textbf{g}^{(p)}_{k}|
	}
\end{align*}

\subsection{Max-SINR Algorithm[Gomadam, 2011]}
For Max-SINR algorithm, we assume \textbf{all channel state information(CSI) is available} to each user. In this method, the solution for each transceivers  are simply Wiener filters. In other words, solving it by Wiener-Hopf equation: $R^{-1}p$ [Adaptive Filter Theory, Simon Haykin]

\subsubsection{Forward\ Training (fix $\textbf{v}^{(c)}_{k}$, $\textbf{v}^{(p)}_{k}$, $\forall k$)}


\begin{align*}
\textbf{g}^{*(c)}_{k} = 
&\bigg[
\textbf{H}_{kk}	\textbf{v}^{(c)}_{k}	\textbf{v}^{H(c)}_{k}	\textbf{H}^{H}_{kk}
+\textbf{H}_{kk}	\textbf{v}^{(p)}_{k}	\textbf{v}^{H(p)}_{k}	\textbf{H}^{H}_{kk}	\\
&+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(c)}_{j})
(\displaystyle\sum_{j \neq k}\textbf{v}^{H(c)}_{j}\textbf{H}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(p)}_{j} \textbf{v}^{H(p)}_{j}\textbf{H}^{H}_{kj})	\\
&+\textbf{H}_{kk}	\textbf{v}^{(c)}_{k}
(\displaystyle\sum_{j \neq k}\textbf{v}^{H(c)}_{j}\textbf{H}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(c)}_{j})
\textbf{v}^{H(c)}_{k}	\textbf{H}^{H}_{kk}
+\sigma^2	\textbf{I}
\bigg]^{-1}	 	(\displaystyle\sum_{i}	\textbf{H}_{ki} 	\textbf{v}^{(c)}_{i})	
\end{align*}

\begin{align*}
\textbf{g}^{*(p)}_{k} = 	
&\bigg[
\textbf{H}_{kk}	\textbf{v}^{(c)}_{k}	\textbf{v}^{H(c)}_{k}	\textbf{H}^{H}_{kk}
+\textbf{H}_{kk}	\textbf{v}^{(p)}_{k}	\textbf{v}^{H(p)}_{k}	\textbf{H}^{H}_{kk}	\\
&+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(c)}_{j})
(\displaystyle\sum_{j \neq k}\textbf{v}^{H(c)}_{j}\textbf{H}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(p)}_{j} \textbf{v}^{H(p)}_{j}\textbf{H}^{H}_{kj})	\\
&+\textbf{H}_{kk}	\textbf{v}^{(c)}_{k}
(\displaystyle\sum_{j \neq k}\textbf{v}^{H(c)}_{j}\textbf{H}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{H}_{kj}\textbf{v}^{(c)}_{j})
\textbf{v}^{H(c)}_{k}	\textbf{H}^{H}_{kk}
+\sigma^2	\textbf{I}
\bigg]^{-1}	 (\textbf{H}_{kk} 	\textbf{v}^{(p)}_{k})
\end{align*}


\subsubsection{Backward\ Training (fix\  $\textbf{g}^{(c)}_{k}$, $\textbf{g}^{(p)}_{k}$, $\forall k$)}


\begin{align*}
\textbf{Z}_{ab}=\textbf{H}^{H}_{ba} 
\end{align*}

\paragraph{without cooperation}


\begin{align*}
\textbf{v}^{*(c)}_{k} = 
&\bigg[
\textbf{Z}_{kk}	\textbf{g}^{(c)}_{k}	\textbf{g}^{H(c)}_{k}	\textbf{Z}^{H}_{kk}
+\textbf{Z}_{kk}	\textbf{g}^{(p)}_{k}	\textbf{g}^{H(p)}_{k}	\textbf{Z}^{H}_{kk}	\\
&+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(c)}_{j})
(\displaystyle\sum_{j \neq k}\textbf{g}^{H(c)}_{j}\textbf{Z}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(p)}_{j}	\textbf{g}^{H(p)}_{j}\textbf{Z}^{H}_{kj})	\\
&+\textbf{Z}_{kk}	\textbf{g}^{(c)}_{k}
(\displaystyle\sum_{j \neq k}\textbf{g}^{H(c)}_{j}\textbf{Z}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(c)}_{j})
\textbf{g}^{H(c)}_{k}	\textbf{Z}^{H}_{kk}
+\sigma^2	\textbf{I}
\bigg]
^{-1}	 (	\displaystyle\sum_{i}   \textbf{Z}_{ki} 	\textbf{g}^{(c)}_{i}	)
\end{align*}






\paragraph{with cooperation}


\begin{align*}
\textbf{V}^{*(c)} =  		
&\left\{ 
\textbf{[Z]}	\textbf{g}^{(c)}	\textbf{g}^{H(c)}		\textbf{[Z]}^{H}
+\textbf{[Z]}
\begin{bmatrix}
       \textbf{g}^{(p)}_{1}	\textbf{g}^{H(p)}_{1} &  &   \textbf{O}      \\[0.3em]
        & \ddots       & \\[0.3em]
           \textbf{O}        & & \textbf{g}^{(p)}_{k}	\textbf{g}^{H(p)}_{k}
     \end{bmatrix}
    \textbf{[Z]}^{H}
    +\sigma^2	\textbf{I}
\right\}
^{-1} ( \textbf{[Z]}  \textbf{g}^{(c)}	)	
\end{align*}

\begin{align*}
\textbf{v}^{*(p)}_{k} =	
 &\bigg[
\textbf{Z}_{kk}	\textbf{g}^{(c)}_{k}	\textbf{g}^{H(c)}_{k}	\textbf{Z}^{H}_{kk}
+\textbf{Z}_{kk}	\textbf{g}^{(p)}_{k}	\textbf{g}^{H(p)}_{k}	\textbf{Z}^{H}_{kk}	\\
&+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(c)}_{j})
(\displaystyle\sum_{j \neq k}\textbf{g}^{H(c)}_{j}\textbf{Z}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(p)}_{j}	\textbf{g}^{H(p)}_{j}\textbf{Z}^{H}_{kj})	\\
&+\textbf{Z}_{kk}	\textbf{g}^{(c)}_{k}
(\displaystyle\sum_{j \neq k}\textbf{g}^{H(c)}_{j}\textbf{Z}^{H}_{kj})
+(\displaystyle\sum_{j \neq k}\textbf{Z}_{kj}\textbf{g}^{(c)}_{j})
\textbf{g}^{H(c)}_{k}	\textbf{Z}^{H}_{kk}
+\sigma^2	\textbf{I}
\bigg]^{-1}	 (\textbf{Z}_{kk}  \textbf{g}^{(p)}_{k})
\end{align*}

\subsection{Bi-Directional Training with LS Algorithm[Shi, 2014]}
For Bi-Directional Training, we assume \textbf{all channel state information(CSI) is not available} to each user. In this case, we could either adopt Least Mean Square Method(LMS) or Least Square Method(LS) to carry out the solution[Adaptive Filter Theory, Simon Haykin]. \textbf{There is actually another interesting finding here}. We all have seen the monotonically decreasing diagram of MSE when designing/studying filters with both methods. We can also show that when the filter converges(achieve minimum MSE), it also maximizes Signal to Interference plus Noise Ratio(SINR). \textbf{However, how does the SINR behave before it converges?} For LS, it is monotonically  decreasing just as MSE diagram, but for LMS, it is \textbf{not!} Therefore, LMS isn't applicable for our application. We need SINR increases after each iteration.


\subsubsection{Forward\ Training (fix\  $\textbf{v}^{(c)}_{k}$, $\textbf{v}^{(p)}_{k}$, $\forall k)$}


\begin{align*}
\textbf{g}^{(c)}_{k}(n+1)= \textbf{g}^{(c)}_{k}(n)+ \mu \textbf y_k(n) [x(n)- \textbf{g}^{H(c)}_{k}(n) \textbf y_k(n)]^*
\end{align*}

\begin{align*}
\textbf{g}^{(p)}_{k}(n+1)= \textbf{g}^{(p)}_{k}(n)+ \mu \textbf y_k(n) [x_k^{(p)}(n)- \textbf{g}^{H(p)}_{k}(n) \textbf y_k(n)]^*
\end{align*}


\subsubsection{Backward\ Training (fix\  $\textbf{g}^{(c)}_{k}$, $\textbf{g}^{(p)}_{k}$, $\forall k$)}
\paragraph{without cooperation}


\begin{align*}
\textbf{v}^{(c)}_{k}(n+1)= \textbf{v}^{(c)}_{k}(n)+ \mu \textbf y_k(n) [x(n)- \textbf{v}^{H(c)}_{k}(n) \textbf y_k(n)]^*
\end{align*}

\begin{align*}
\textbf{v}^{(p)}_{k}(n+1)= \textbf{v}^{(p)}_{k}(n)+ \mu \textbf y_k(n) [x_k^{(p)}(n)- \textbf{v}^{H(p)}_{k}(n) \textbf y_k(n)]^*
\end{align*}


\paragraph{with cooperation}


\begin{align*}
\textbf{V}^{(c)}(n+1)= \textbf{V}^{(c)}(n)+ \mu \textbf Y(n) [x(n)- \textbf{V}^{H(c)}(n) \textbf Y(n)	]^*
\end{align*}

\begin{align*}
\textbf{v}^{(p)}_{k}(n+1)= \textbf{v}^{(p)}_{k}(n)+ \mu \textbf y_k(n) [x_k^{(p)}(n)- \textbf{v}^{H(p)}_{k}(n) \textbf y_k(n)]^*
\end{align*}

\subsection{Special Case(2 Users,  MIMO Channel, Only Common Messages)}
\begin{align*}
\textbf{y}_{k} &=	\displaystyle\sum_{i=1}^{2}\textbf{H}_{ki}\textbf{v}^{(c)}_{i}x	+	\textbf{n}_{k}
\end{align*}

\begin{align*}
MSE^{(c)}_{k} &= \mathrm{E}	[	(	x-\textbf{g}^{H(c)}_{k}	\textbf{y}_{k}	)(x-\textbf{g}^{H(c)}_{k}	\textbf{y}_{k})^{H}	] \\
		       & = \mathrm{E}[x^{2}]	 - \mathrm{E}[x	\textbf{y}_{k}^{H} \textbf{g}^{(c)}_{k}]	
		       					-  \mathrm{E}[x	\textbf{g}_{k}^{H(c)} \textbf{y}_{k}]
							+ \mathrm{E}[\textbf{g}_{k}^{H(c)} \textbf{y}_{k}	\textbf{y}_{k}^{H} \textbf{g}^{(c)}_{k}]			\\
		       & = 1 - \displaystyle\sum_{i=1}^{2}	\textbf{v}^{H(c)}_{i}	\textbf{H}^{H}_{ki}	\textbf{g}^{(c)}_{k}
		       		- \textbf{g}^{H(c)}_{k}		\displaystyle\sum_{i=1}^{2}	\textbf{H}_{ki}	\textbf{v}^{(c)}_{i}
				+\textbf{g}^{H(c)}_{k}		(\displaystyle\sum_{i=1}^{2}\textbf{H}_{ki}\textbf{v}^{(c)}_{i})
									(\displaystyle\sum_{i=1}^{2}\textbf{v}^{H(c)}_{i}\textbf{H}^{H}_{ki})	\textbf{g}^{(c)}_{k}
				+\sigma^2		\textbf{g}^{H(c)}_{k}	 \textbf{g}^{(c)}_{k}
\end{align*}

\begin{align*}
\textbf{v}^{*(c)}_{1}  &= \operatornamewithlimits{argmin}_{\textbf{v}^{(c)}_{1}}	(\displaystyle\sum_{k=1}^{2}	MSE^{(c)}_{k})	\\
			       &=\bigg[ 2	\textbf{H}^{H}_{11}	\textbf{g}^{(c)}_{1}	\textbf{g}^{H(c)}_{1}		\textbf{H}_{11}
				   +	2	\textbf{H}^{H}_{21}	\textbf{g}^{(c)}_{2}	\textbf{g}^{H(c)}_{2}		\textbf{H}_{21})
				   \bigg]^{-1} 
				   (2	\textbf{H}^{H}_{11}	\textbf{g}^{(c)}_{1}	+	2	\textbf{H}^{H}_{21}	\textbf{g}^{(c)}_{2}	\\
				   &\ \ \ \ -	\textbf{g}^{H(c)}_{1}	\textbf{H}_{12}	\textbf{v}^{(c)}_{2} 	\textbf{H}^{H}_{11}	\textbf{g}^{(c)}_{1}
				   -	\textbf{H}^{H}_{11}	\textbf{g}^{(c)}_{1}	\textbf{v}^{H(c)}_{2}	\textbf{H}^{H}_{12}	\textbf{g}^{(c)}_{1}
				   -	\textbf{g}^{H(c)}_{2}	\textbf{H}_{22}	\textbf{v}^{(c)}_{2} 	\textbf{H}^{H}_{21}	\textbf{g}^{(c)}_{2}
				   -	\textbf{H}^{H}_{21}	\textbf{g}^{(c)}_{2}	\textbf{v}^{H(c)}_{2}	\textbf{H}^{H}_{22}	\textbf{g}^{(c)}_{2}
				   )
\end{align*}

\begin{align*}
\textbf{v}^{*(c)}_{2} 	&= \operatornamewithlimits{argmin}_{\textbf{v}^{(c)}_{2}}	(\displaystyle\sum_{k=1}^{2}	MSE^{(c)}_{k})	\\
				   &= 	\bigg[ 2	\textbf{H}^{H}_{12}	\textbf{g}^{(c)}_{1}	\textbf{g}^{H(c)}_{1}		\textbf{H}_{12}
				   +	2	\textbf{H}^{H}_{22}	\textbf{g}^{(c)}_{2}	\textbf{g}^{H(c)}_{2}		\textbf{H}_{22})
				   \bigg]^{-1} 
				   (2	\textbf{H}^{H}_{12}	\textbf{g}^{(c)}_{1}	+	2	\textbf{H}^{H}_{22}	\textbf{g}^{(c)}_{2}	\\
				   &\ \ \ \ -	\textbf{g}^{H(c)}_{1}	\textbf{H}_{11}	\textbf{v}^{(c)}_{1} 	\textbf{H}^{H}_{12}	\textbf{g}^{(c)}_{1}
				   -	\textbf{H}^{H}_{12}	\textbf{g}^{(c)}_{1}	\textbf{v}^{H(c)}_{1}	\textbf{H}^{H}_{11}	\textbf{g}^{(c)}_{1}
				   -	\textbf{g}^{H(c)}_{2}	\textbf{H}_{21}	\textbf{v}^{(c)}_{1} 	\textbf{H}^{H}_{22}	\textbf{g}^{(c)}_{2}
				   -	\textbf{H}^{H}_{22}	\textbf{g}^{(c)}_{2}	\textbf{v}^{H(c)}_{1}	\textbf{H}^{H}_{21}	\textbf{g}^{(c)}_{2}
				   )
\end{align*}












\newpage

\section{System Model 2 : Cooperative Transmitters }

 \begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{forward_channel_2}}
    \caption{Forward Channel}
\end{figure} 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{backward_channel_2}}
    \caption{Backward Channel}
\end{figure} 

\subsection{Optimization Problem}
\begin{align*}
\min_{\textbf{v}_{k}^{(j)} ,\textbf{g}_{k}} \displaystyle\sum_{k} 	w_k	MSE_{k}	
\end{align*}

\begin{align*}
\text{subject to}  \ \displaystyle\sum_{j}	\|	\textbf{v}^{(j)}_{k}	\|^{2} = P; \| \textbf{g}_{k}	\|^{2} = P
\end{align*}

\subsection{The received signal vector at k-th receiver}
\begin{align*}
\textbf{y}_{k}  = \displaystyle\sum_{i}		\bigg[	\textbf{H}_{ki}	\displaystyle\sum_{j}	(\textbf{v}^{(j)}_{i}	x_j)		\bigg]	+\textbf{n}_{k}
\end{align*}


\subsection{SINR Derivation}
\begin{align*}
s_{k} = \textbf{g}^{H}_{k}
		(\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(k)}_{i}x_k)
\end{align*}



\begin{align*}
n_{k} = \textbf{g}^{H}_{k}
		\bigg[\displaystyle\sum_{i}
		\big(	\textbf{H}_{ki} 
		\displaystyle\sum_{j \neq k}	\textbf{v}^{(j)}_{i}x_j	\big)
		+\textbf{n}_{k}\bigg]
\end{align*}

\begin{align*}
\frac	{	|s_{k}|^2	}{	|n_{k}|^2	} = 
\frac {	|\textbf{g}^{H}_{k}
		\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(k)}_{i}|^2	
	} 
	{	\displaystyle\sum_{j \neq k}
		|\textbf{g}^{H}_{k}
		\displaystyle\sum_{i}
		\textbf{H}_{ki} 
		\textbf{v}^{(j)}_{i}|^2
		+|\textbf{g}^{H}_{k}
		\textbf{R}_{k}
		\textbf{g}_{k}|
	}
\end{align*}

\subsection{Forward Direction}
The solution is simply $R^{-1}p$.


\begin{align*}
\textbf{g}^*_{k} 	&= \operatornamewithlimits{argmin}_{\textbf{g}_{k}}	(\displaystyle\sum_{k} 	\bigg[	MSE_{k}	+	\lambda_{i}	(\displaystyle\sum_{j}	\|	\textbf{v}^{(j)}_{i}	\|	-P	)	\bigg])	\\
			&= \bigg[	\displaystyle\sum_{j}
				\big[	(\displaystyle\sum_{i}\textbf{H}_{ki}\textbf{v}^{(j)}_{i})
			      (\displaystyle\sum_{i}\textbf{v}^{H(j)}_{i}\textbf{H}^{H}_{ki})	\big]	 +\sigma^2	\textbf{I}
			      \bigg]^{-1}	(\displaystyle\sum_{i}	\textbf{H}_{ki}	\textbf{v}^{(k)}_{i})		      
\end{align*}


\subsection{Backward Direction: Iterative Method which Doesn't Work}

In this method, we found a solution for each $\textbf{v}_{k}^{(j)}$ depending on some $\textbf{v}_{x}^{(j)}$, where k $\neq$ x. For example, $\textbf{v}_{1}^{(1)}$ depends on $\textbf{v}_{2}^{(1)}$. Then, we try to solve the optimization problem by iterating between $\textbf{v}_{k}^{(j)}$ and $\textbf{v}_{x}^{(j)}$, and hope it will work. However, our numerical experiment shows that it will not converge for some channels.
\begin{align*}
MSE_{k} &= \mathrm{E}	[	(	x_k-\textbf{g}^{H}_{k}	\textbf{y}_{k}	)(x_k-\textbf{g}^{H}_{k}	\textbf{y}_{k})^{H}	] \\
		       & = \mathrm{E}[x_k^{2}]	 - \mathrm{E}[x_k	\textbf{y}_{k}^{H} \textbf{g}_{k}]	
		       					-  \mathrm{E}[x_k	\textbf{g}_{k}^{H} \textbf{y}_{k}]
							+ \mathrm{E}[\textbf{g}_{k}^{H} \textbf{y}_{k}	\textbf{y}_{k}^{H} \textbf{g}_{k}]			\\
		       & = 1 - \displaystyle\sum_{i}	\textbf{v}^{H(k)}_{i}	\textbf{H}^{H}_{ki}	\textbf{g}_{k}
		       		- \textbf{g}^{H}_{k}		\displaystyle\sum_{i}	\textbf{H}_{ki}	\textbf{v}^{(k)}_{i}
				+\textbf{g}^{H}_{k}		\displaystyle\sum_{j}
									\bigg[	(\displaystyle\sum_{i}\textbf{H}_{ki}\textbf{v}^{(j)}_{i})
									(\displaystyle\sum_{i}\textbf{v}^{H(j)}_{i}\textbf{H}^{H}_{ki})	\bigg]	\textbf{g}_{k}
				+\sigma^2		\textbf{g}^{H}_{k}	 \textbf{g}_{k}
\end{align*}



\begin{align*}
\textbf{v}^{*(1)}_{1}  &= \operatornamewithlimits{argmin}_{\textbf{v}^{(1)}_{1}}	(\displaystyle\sum_{k=1,2} 	\bigg[	MSE_{k}	+	\lambda_{k}	(\displaystyle\sum_{j=1,2}	\|	\textbf{v}^{(j)}_{k}	\|	-P	)	\bigg])	\\
			       &=\bigg[ 2	\textbf{H}^{H}_{11}	\textbf{g}_{1}	\textbf{g}^{H}_{1}		\textbf{H}_{11}	w_1
				   +	2	\textbf{H}^{H}_{21}	\textbf{g}_{2}	\textbf{g}_{2}		\textbf{H}_{21}	w_2	+2	\lambda^{*}_1 I
				   \bigg]^{-1} 
				   (
				   2	\textbf{H}^{H}_{11}	\textbf{g}_{1}	w_1	
				   -\textbf{g}^{H}_{1}	\textbf{H}_{12}	\textbf{v}^{(1)}_{2} 	\textbf{H}^{H}_{11}	\textbf{g}_{1}	w_1	\\
				   &\ \ \ \ 
				   -	\textbf{H}^{H}_{11}	\textbf{g}_{1}	\textbf{v}^{H(1)}_{2}	\textbf{H}^{H}_{12}	\textbf{g}_{1}	w_1
				   -	\textbf{g}^{H}_{2}	\textbf{H}_{22}	\textbf{v}^{(1)}_{2} 	\textbf{H}^{H}_{21}	\textbf{g}_{2}	w_2
				   -	\textbf{H}^{H}_{21}	\textbf{g}_{2}	\textbf{v}^{H(1)}_{2}	\textbf{H}^{H}_{22}	\textbf{g}_{2}	w_2
				   )\\
			      &=\bigg[	2	\mathrm{E}[x^*_1	\textbf{y}_1]	\mathrm{E}[x^*_1	\textbf{y}_1]^H	w_1
			      			+	2	\mathrm{E}[x^*_2	\textbf{y}_1]	\mathrm{E}[x^*_2	\textbf{y}_1]^H	w_2	+2	\lambda^{*}_1 I
			      	  \bigg]^{-1} 
				  (2	\mathrm{E}[x^*_1	\textbf{y}_1]	w_1
				  -\mathrm{E}[x^*_1	\textbf{y}_2]^H	\textbf{v}^{(1)}_{2} 	\mathrm{E}[x^*_1	\textbf{y}_1]	w_1	\\
				  &\ \ \ \ 	
				  -\mathrm{E}[x^*_1	\textbf{y}_1]	\textbf{v}^{H(1)}_{2} 	\mathrm{E}[x^*_1	\textbf{y}_2]	w_1	
				  -\mathrm{E}[x^*_2	\textbf{y}_2]^H	\textbf{v}^{(1)}_{2} 	\mathrm{E}[x^*_2	\textbf{y}_1]	w_2
				   -\mathrm{E}[x^*_2	\textbf{y}_1]	\textbf{v}^{H(1)}_{2} 	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2	
				   )
\end{align*}







\begin{align*}
\textbf{v}^{*(1)}_{2} 	&= \operatornamewithlimits{argmin}_{\textbf{v}^{(1)}_{2}}	(\displaystyle\sum_{k=1,2} 	\bigg[	MSE_{k}	+	\lambda_{k}	(\displaystyle\sum_{j=1,2}	\|	\textbf{v}^{(j)}_{k}	\|	-P	)	\bigg])	\\
				   &= 	\bigg[ 2	\textbf{H}^{H}_{12}	\textbf{g}_{1}	\textbf{g}^{H}_{1}		\textbf{H}_{12}	w_1
				   +	2	\textbf{H}^{H}_{22}	\textbf{g}_{2}	\textbf{g}^{H}_{2}		\textbf{H}_{22}	w_2	+2	\lambda^{*}_2 I)
				   \bigg]^{-1} 
				   (2	\textbf{H}^{H}_{12}	\textbf{g}_{1}	w_1		
				   -	\textbf{g}^{H}_{1}	\textbf{H}_{11}	\textbf{v}^{(1)}_{1} 	\textbf{H}^{H}_{12}	\textbf{g}_{1} w_1	\\
				   &\ \ \ \ 
				   -	\textbf{H}^{H}_{12}	\textbf{g}_{1}	\textbf{v}^{H(1)}_{1}	\textbf{H}^{H}_{11}	\textbf{g}_{1}	w_1
				   -	\textbf{g}^{H}_{2}	\textbf{H}_{21}	\textbf{v}^{(1)}_{1} 	\textbf{H}^{H}_{22}	\textbf{g}_{2}	w_2
				   -	\textbf{H}^{H}_{22}	\textbf{g}_{2}	\textbf{v}^{H(1)}_{1}	\textbf{H}^{H}_{21}	\textbf{g}_{2}	w_2
				   )\\
				   &=\bigg[	2	\mathrm{E}[x^*_1	\textbf{y}_2]	\mathrm{E}[x^*_1	\textbf{y}_2]^H	w_1
			      			+	2	\mathrm{E}[x^*_2	\textbf{y}_2]	\mathrm{E}[x^*_2	\textbf{y}_2]^H	w_2	+2	\lambda^{*}_2 I
			      	  \bigg]^{-1} 
				  (2	\mathrm{E}[x^*_1	\textbf{y}_2]	w_1
				  -\mathrm{E}[x^*_1	\textbf{y}_1]^H	\textbf{v}^{(1)}_{1} 	\mathrm{E}[x^*_1	\textbf{y}_2]	w_1	\\
				  &\ \ \ \ 	
				  -\mathrm{E}[x^*_1	\textbf{y}_2]	\textbf{v}^{H(1)}_{1} 	\mathrm{E}[x^*_1	\textbf{y}_1]	w_1	
				  -\mathrm{E}[x^*_2	\textbf{y}_1]^H	\textbf{v}^{(1)}_{1} 	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2
				   -\mathrm{E}[x^*_2	\textbf{y}_2]	\textbf{v}^{H(1)}_{1} 	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2	
				   )
\end{align*}


\begin{align*}
\textbf{v}^{*(2)}_{1}  &= \operatornamewithlimits{argmin}_{\textbf{v}^{(2)}_{1}}	(\displaystyle\sum_{k=1,2} 	\bigg[	MSE_{k}	+	\lambda_{k}	(\displaystyle\sum_{j=1,2}	\|	\textbf{v}^{(j)}_{k}	\|	-P	)	\bigg])	\\
			       &=\bigg[ 2	\textbf{H}^{H}_{11}	\textbf{g}_{1}	\textbf{g}^{H}_{1}		\textbf{H}_{11}	w_1
				   +	2	\textbf{H}^{H}_{21}	\textbf{g}_{2}	\textbf{g}_{2}		\textbf{H}_{21}	w_2	+2	\lambda^{*}_1 I
				   \bigg]^{-1} 
				   (
				   2	\textbf{H}^{H}_{21}	\textbf{g}_{2}	w_2	
				   -\textbf{g}^{H}_{1}	\textbf{H}_{12}	\textbf{v}^{(2)}_{2} 	\textbf{H}^{H}_{11}	\textbf{g}_{1}	w_1	\\
				   &\ \ \ \ 
				   -	\textbf{H}^{H}_{11}	\textbf{g}_{1}	\textbf{v}^{H(2)}_{2}	\textbf{H}^{H}_{12}	\textbf{g}_{1}	w_1
				   -	\textbf{g}^{H}_{2}	\textbf{H}_{22}	\textbf{v}^{(2)}_{2} 	\textbf{H}^{H}_{21}	\textbf{g}_{2}	w_2
				   -	\textbf{H}^{H}_{21}	\textbf{g}_{2}	\textbf{v}^{H(2)}_{2}	\textbf{H}^{H}_{22}	\textbf{g}_{2}	w_2
				   )\\
			      &=\bigg[	2	\mathrm{E}[x^*_1	\textbf{y}_1]	\mathrm{E}[x^*_1	\textbf{y}_1]^H	w_1
			      			+	2	\mathrm{E}[x^*_2	\textbf{y}_1]	\mathrm{E}[x^*_2	\textbf{y}_1]^H	w_2	+2	\lambda^{*}_1 I
			      	  \bigg]^{-1} 
				  (2	\mathrm{E}[x^*_2	\textbf{y}_1]	w_2
				  -\mathrm{E}[x^*_1	\textbf{y}_2]^H	\textbf{v}^{(2)}_{2} 	\mathrm{E}[x^*_1	\textbf{y}_1]	w_1	\\
				  &\ \ \ \ 	
				  -\mathrm{E}[x^*_1	\textbf{y}_1]	\textbf{v}^{H(2)}_{2} 	\mathrm{E}[x^*_1	\textbf{y}_2]	w_1	
				  -\mathrm{E}[x^*_2	\textbf{y}_2]^H	\textbf{v}^{(2)}_{2} 	\mathrm{E}[x^*_2	\textbf{y}_1]	w_2
				   -\mathrm{E}[x^*_2	\textbf{y}_1]	\textbf{v}^{H(2)}_{2} 	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2	
				   )
\end{align*}


\begin{align*}
\textbf{v}^{*(2)}_{2} 	&= \operatornamewithlimits{argmin}_{\textbf{v}^{(2)}_{2}}	(\displaystyle\sum_{k=1,2} 	\bigg[	MSE_{k}	+	\lambda_{k}	(\displaystyle\sum_{j=1,2}	\|	\textbf{v}^{(j)}_{k}	\|	-P	)	\bigg])	\\
				   &= 	\bigg[ 2	\textbf{H}^{H}_{12}	\textbf{g}_{1}	\textbf{g}^{H}_{1}		\textbf{H}_{12}	w_1
				   +	2	\textbf{H}^{H}_{22}	\textbf{g}_{2}	\textbf{g}^{H}_{2}		\textbf{H}_{22}	w_2	+2	\lambda^{*}_2 I)
				   \bigg]^{-1} 
				   (2	\textbf{H}^{H}_{22}	\textbf{g}_{2}	w_2		
				   -	\textbf{g}^{H}_{1}	\textbf{H}_{11}	\textbf{v}^{(2)}_{1} 	\textbf{H}^{H}_{12}	\textbf{g}_{1} w_1	\\
				   &\ \ \ \ 
				   -	\textbf{H}^{H}_{12}	\textbf{g}_{1}	\textbf{v}^{H(2)}_{1}	\textbf{H}^{H}_{11}	\textbf{g}_{1}	w_1
				   -	\textbf{g}^{H}_{2}	\textbf{H}_{21}	\textbf{v}^{(2)}_{1} 	\textbf{H}^{H}_{22}	\textbf{g}_{2}	w_2
				   -	\textbf{H}^{H}_{22}	\textbf{g}_{2}	\textbf{v}^{H(2)}_{1}	\textbf{H}^{H}_{21}	\textbf{g}_{2}	w_2
				   )\\
				   &=\bigg[	2	\mathrm{E}[x^*_1	\textbf{y}_2]	\mathrm{E}[x^*_1	\textbf{y}_2]^H	w_1
			      			+	2	\mathrm{E}[x^*_2	\textbf{y}_2]	\mathrm{E}[x^*_2	\textbf{y}_2]^H	w_2	+2	\lambda^{*}_2 I
			      	  \bigg]^{-1} 
				  (2	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2
				  -\mathrm{E}[x^*_1	\textbf{y}_1]^H	\textbf{v}^{(2)}_{1} 	\mathrm{E}[x^*_1	\textbf{y}_2]	w_1	\\
				  &\ \ \ \ 	
				  -\mathrm{E}[x^*_1	\textbf{y}_2]^H	\textbf{v}^{H(2)}_{1} 	\mathrm{E}[x^*_1	\textbf{y}_1]	w_1	
				  -\mathrm{E}[x^*_2	\textbf{y}_1]^H	\textbf{v}^{(2)}_{1} 	\mathrm{E}[x^*_2	\textbf{y}_2]	w_2
				   -\mathrm{E}[x^*_2	\textbf{y}_2]^H	\textbf{v}^{H(2)}_{1} 	\mathrm{E}[x^*_2	\textbf{y}_1]	w_2	
				   )
\end{align*}




\subsection{Backward Direction: Duality Method}
For the duality method, if the optimization problem is convex, then the optimal solution is guaranteed. If it is not, there will exist a \textbf{duality gap} which will not give the optimal solution[Convex Optimization, Boyd]. However, our problem is convex. Thus we can get the \textbf{exact/global optimal} solution.

\subsubsection{Convexity Analysis}

We could rewrite our cost function in
\begin{align*}
\displaystyle\sum_{k} 	w_k	MSE_{k}	=  (\displaystyle\sum_{k} w_k)-\textbf{kv}-(\textbf{kv})^{H}+\textbf{v}^{H}\textbf{A}\textbf{v}+ \sigma^{2} \textbf{g}^{H} \textbf{g}
\end{align*}

where \textbf{k} is a constant row vector with channel information, \textbf{A} is a constant matrix with channel information, and $\pmb{\lambda}$ is a diagonal matrix with each $\pmb{\lambda}_{ii}$ to be some $\lambda_{k}$. Since A is a \textbf{positive definite} matrix, the cost function is convex.




\subsubsection{Primal Problem}

\begin{align*}
\min_{\textbf{v}_{k}^{(j)}}\
L =\displaystyle\sum_{k} 	\bigg[w_k	MSE_{k}	+	\lambda_{k}	(\displaystyle\sum_{j}	\|	\textbf{v}^{(j)}_{k}	\|	-P	)	\bigg]
\end{align*}

We get 
\begin{align*}
\textbf{v}^{*H} = \textbf{k}[\textbf{A}+\pmb{\lambda}]^{-1}, \forall \lambda_{k} \geq 0
\end{align*}
The \textbf{feasible set} of $\lambda_{k}$ is positive number set. The reason could be found in textbook. There's an interesting result of $\lambda_{k}$. This is, if we \textbf{change our constraint from equality to inequality}, the duality method will give the same answer. Although this is not too surprising, it is justified by mathematics.






\subsubsection{Dual Problem}

\begin{align*}
\max_{	\pmb{\lambda} \geq \textbf{0}	}	G( \textbf{v}^{*}, \pmb{\lambda} ) \ = \max_{	\pmb{\lambda}	\geq \textbf{0}}\ (w1+w2+w3) - \textbf{v}^{*H}\textbf{k}^{H} + \sigma^{2} \textbf{g}^{H} \textbf{g} - P(\lambda_{1}+\lambda_{2}+\lambda_{3})
\end{align*}

We could actually find the \textbf{analytic solution} for $\pmb{\lambda}$. However, for 3 users case, the process includes finding the \textbf{inverse of an 6x6 symbolic matrix}  which is \textbf{solvable} but \textbf{complicated}, let alone cases above 3 users. Thus, we choose to solve the dual problem with \textbf{numerical method}. By the optimization theory, dual problems are \textbf{always} convex. Therefore, we could get the optimal $\pmb{\lambda}$ simply with gradient algorithm. The gradient is shown below.

\begin{align*}
\frac{\partial G( \textbf{v}^{*}, \pmb{\lambda} )}{\partial \lambda_{k}} = 
-P+
\Tr[
(A+\lambda_k\textbf{I})^{-T}\textbf{k}^{H}\textbf{k}(A+\lambda_k\textbf{I})^{-T}
]
\end{align*}

The partial derivatives of each $\lambda_{k}$ are mutually independent, and have the same formula. This implies that it's easy to generalize the result for more users.


\newpage

\section{System Model 3 : Augmented Receivers}
In this model, we make each receivers(mobile station) possess the ability to assist channel estimation in backward direction. Since it is rather easy to add a filter to receivers, if this method does improve the capacity, it will be extremely fascinating. We solve each transceivers with Wiener-Hopf equation: $R^{-1}p$.

 \begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{forward_channel_3}}
    \caption{Forward Channel}
\end{figure} 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{backward_channel_3}}
    \caption{Backward Channel}
\end{figure} 

\subsection{Optimization Problem}
\begin{align*}
\min_{\textbf{v}_{k}^{(j)} ,\textbf{g}_{k}} \displaystyle\sum_{k} 	w_k	MSE_{k}	
\end{align*}

\begin{align*}
\text{subject to}  \ \displaystyle\sum_{j}	\|	\textbf{v}^{(j)}_{k}	\|^{2} = P; \| \textbf{g}_{k}	\|^{2} = P
\end{align*}

\subsection{Numerical Results}
This is a 3 users, and 2X2 MIMO interference channel, with SNR equals to 20dB. The results are averaged over 1000 complex gaussian channels. X-axis represents the number of iterations: start from backward direction and end in forward direction. Y-axis represents the sum capacity of three users. The first thing we can tell is the \textbf{augmented one} is a lot worse than the \textbf{simple one}(Only one filter at each transceivers. For example, for 2 users case, only have $v^{(1)}_1$,$v^{(2)}_2$,$g^{(1)}_1$,$g^{(2)}_2$ ). \textbf{This is because transmitters distribute all messages to all users.} In the experiment, transmitter 1 sends 2 bits of message 1 to user 1, 2, and 3. However user 2 and 3 simply drop message 1 off(they don't need it). Thus, the sum capacity of augmented receivers only have 1/3 of capacity compared with the simple one.

 \begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{Simple_Receivers}}
    \caption{Simple Receivers}
\end{figure} 

\begin{figure}[h]
    \centering
    \centerline{\includegraphics[width=110mm]{Augmented_Receivers}}
    \caption{Augmented Receivers}
\end{figure} 





\newpage

9. Numerical Simulation(2 Users, 2X2 MIMO Channel)


\begin{align*}
Rayleigh\ Fading\ Channel
\end{align*}

\begin{align*}
\ Cross\ Channel\ Gain = 0.8*Direct\ Channel\ Gain
\end{align*}

\begin{align*}
SNR = \frac {1}{\sigma^2} = 10^3=30dB
\end{align*}

\begin{align*}
Observation1: &If\ the\ training\ length\ is\ long\ enough,\ each\ LMS\ filter\ (\textbf{v}^{(c)}_{k},\textbf{v}^{(p)}_{k},\textbf{g}^{(c)}_{k},\textbf{g}^{(p)}_{k},)\\
&will\ converge\ to\ Wiener\ filter
\end{align*}

\begin{align*}
Observation2:\ Use\ Wiener\ filters,\ and\ only\ send\ common\ messages.\ Sum\ rate\ C\ =\ 11.6\ bit/channel
\end{align*}

\begin{align*}
Observation3:\ Use\ Wiener\ filters,\ and\ only\ send\ private\ messages.\ Sum\ rate\ C\ =\ 2.63\ bit/channel
\end{align*}

\begin{align*}
Observation4:\ Use\ Wiener\ filters,\ and\  send\ both\ messages.\ Sum\ rate\ C\ =\ 3.35\ bit/channel
\end{align*}

\begin{align*}
Observation5:\ Under\ the\ cooperation\ scheme,\ transmitters\ don't\ converge\ to\ Wiener\ filters 
\end{align*}

\begin{align*}
Observation5:\ Under\ the\ cooperation\ scheme,\ C=3.15\ bit/channel
\end{align*}



\begin{figure}[bp!]
    \centering
    \includegraphics[width=150mm]{01}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{1USER_4X4MIMO}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LMS1}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LMS2}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LMS3}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LMS4}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LMS5}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{LS1}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{only_private}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{no_coop}}
    \caption{Insert caption}
\end{figure} 

\begin{figure}[bp!]
    \centering
    \centerline{\includegraphics[width=220mm]{coop}}
    \caption{Insert caption}
\end{figure} 

 


\end{document}  